[build-system]
requires = [
    "setuptools >= 64",
]
build-backend = "setuptools.build_meta:__legacy__"

[project]
name = "gptqmodel"
dynamic = ["version"]
description = "Production ready LLM model compression/quantization toolkit with hw accelerated inference support for both cpu/gpu via HF, vLLM, and SGLang."
readme = "README.md"
requires-python = ">=3.11"
license-files = ["licenses/LICENSE.apache"]
authors = [
    { name = "ModelCloud", email = "qubitium@modelcloud.ai" },
]
keywords = ["gptq", "awq", "qqq", "autogptq", "autoawq", "eora", "gar", "quantization", "large-language-models", "transformers", "llm", "moe", "compression"]
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Programming Language :: C++",
    "Intended Audience :: Developers",
    "Intended Audience :: Education",
    "Intended Audience :: Science/Research",
    "Intended Audience :: Information Technology",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Scientific/Engineering :: Information Analysis",
]
dependencies = [
    "accelerate>=1.10.1",
    # "datasets>=3.5.0",
    "numpy==2.2.6",
    "torch>=2.8.0",
    "safetensors>=0.6.2",
    "transformers>=4.56.0",
    "threadpoolctl>=3.6.0",
    "packaging>=24.2",
    "device-smi>=0.4.1",
    "protobuf>=6.32.0",
    "pillow>=11.3.0",
    "hf_transfer>=0.1.9",
    "huggingface_hub>=0.34.4",
    "random_word>=1.0.13",
    "tokenicer>=0.0.5",
    "logbar>=0.0.4",
    # "soundfile==0.13.1",   # Qwen-Omni dependency
    "wheel>=0.45.1",
    "memlord>=0.0.1",
]

[project.urls]
Homepage = "https://github.com/ModelCloud/GPTQModel"

[project.optional-dependencies]
test = [
    "pytest>=8.3.5",
    "parameterized",
]
quality = [
    "ruff==0.13.0",
    "isort==6.0.1",
]
vllm = [
    "vllm>=0.10.2",
    "flashinfer-python>=0.3.1",
]
sglang = [
    "sglang[srt]>=0.4.6",
    "flashinfer-python>=0.3.1",
]
bitblas = [
    "bitblas==0.0.1-dev13",
]
hf = [
    "optimum>=1.21.2",
]
logger = [
    "clearml",
    "random_word",
    "plotly",
]
eval = [
    "lm_eval>=0.4.7",
    "evalplus>=0.3.1",
]
triton = [
    "triton>=3.4.0",
]
openai = [
    "uvicorn",
    "fastapi",
    "pydantic",
]
mlx = [
    "mlx_lm>=0.24.0",
]
